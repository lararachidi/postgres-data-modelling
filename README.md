# Data Modelling with Postgres

## Summary

The purpose of this repo is to create a Postgres database with tables designed to optimize queries on song play analysis. The goal is to create a database schema and ETL pipeline for this analysis.

The raw data is in two directories: a directory of JSON logs on user activity on the app, and a directory with JSON metadata on the songs. The ETL pipeline  transfers data from files in these two directories into fact and dimension tables in Postgres using Python and SQL.

### Run python scripts

To create the Postgres schema, run the following processes in order from your terminal in the root directory:

1. Create the Postgres tables with the `create_tables` python script:   
  `python create_tables.py`

2. Run the data modelling pipeline with the `etl` python script:  
  `python etl.py`

## Datasets

### Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 

For example, here is the filepath to a file in this dataset: song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

### Log Dataset 

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here is the filepath to a file in this dataset:
log_data/2018/11/2018-11-12-events.json

## Files     

1. `test.ipynb` displays the first few rows of each table to check the database.
2. `create_tables.py` drops and creates tables. Run this file to reset your tables before each time you run your ETL scripts.
3. `etl.ipynb` reads and processes a single file from `song_data` and `log_data` and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.
4. `etl.py` reads and processes files from `song_data` and `log_data` and loads them into your tables. 
5. `sql_queries.py` contains all your sql queries, and is imported into the last three files above.

## Database Schema 


![ERD](ERD.png)
